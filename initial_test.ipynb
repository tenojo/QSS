{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57f15f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from scripts import dataset_scripts as ds_s, mauve_quantization as mq, subset_selection as ss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from datasets import Dataset, load_from_disk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e753dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicts are in format: {ds_path, ds_name, under_ds_name}\n",
    "#human_ds_dict = {\"ds_path\":\"data/human/news-fi-2019.jsonl\", \"ds_name\":\"news-fi-2019.jsonl\", \"under_ds_name\":None}\n",
    "#clums_ds_dict = {\"ds_path\":\"data/clumsified/news-fi-2019.jsonl_regeneration_5_mini_regen_round_1.jsonl\", \"ds_name\":\"news-fi-2019.jsonl_regeneration_5_mini_regen_round_1.jsonl\", \"under_ds_name\":\"news-fi-2019.jsonl\"}\n",
    "\n",
    "\n",
    "#ds = ds_s.format_datasets([human_ds_dict, clums_ds_dict])\n",
    "#ref, remaining = ds_s.sample_reference_corpus(ds, \"news-fi-2019.jsonl\", 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3dfcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/scratch/project_2000539/tapio/HF_cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d84c632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model name that can be loaded from HF goes here\n",
    "model_id = model_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "\n",
    "# ## Getting the embeddings with the wanted (L)LM\n",
    "\n",
    "#inputs_q = [tokenizer.encode(x['text'], return_tensors=\"pt\", truncation=True, max_length=512) for x in remaining]\n",
    "#embeddings_q = mq.featurize_tokens_from_model(model, inputs_q, 1, name=\"\", verbose=False)\n",
    "#inputs_q = []\n",
    "#del inputs_q\n",
    "#inputs_p = [tokenizer.encode(x['text'], return_tensors=\"pt\", truncation=True, max_length=512) for x in ref]\n",
    "#embeddings_p = mq.featurize_tokens_from_model(model, inputs_p, 1, name=\"\", verbose=False)\n",
    "#inputs_p = []\n",
    "#del inputs_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4eac84-8b4c-4f5e-8732-f27d49711691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(ref)):\n",
    "#    ref[i]['embedding']= embeddings_p[i]\n",
    "\n",
    "#for i in range(len(remaining)):\n",
    "#    remaining[i]['embedding']= embeddings_q[i]\n",
    "\n",
    "\n",
    "#torch.save(ref, 'data/embs/ref.pt')\n",
    "\n",
    "#ds_test = Dataset.from_list(ref)\n",
    "#ds_test.save_to_disk(\"data/embs/ref.hf\")\n",
    "\n",
    "#ds_test = Dataset.from_list(remaining)\n",
    "#ds_test.save_to_disk(\"data/embs/remaining.hf\")\n",
    "\n",
    "#torch.load(remaining, 'data/embs/remaining.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61c1c461-d073-4354-9d99-3a21d47046b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ds = load_from_disk(\"data/embs/ref.hf\")\n",
    "remaining_ds = load_from_disk(\"data/embs/remaining.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "828b5f2e-c63e-4154-98dd-af6527bb2f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = ref_ds.to_list()\n",
    "remaining = remaining_ds.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca1012d8-1dcf-4cb0-aa7d-91cf2539c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_p = [torch.tensor(x['embedding']) for x in ref]\n",
    "embeddings_q = [torch.tensor(x['embedding']) for x in remaining]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6834fd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of clusters is 500\n",
      "seed = 0\n",
      "performing clustering in lower dimension = 392\n",
      "'Shape of embeddings before PCA transformation (43030, 1024)'\n",
      "'Shape of embeddings after PCA transformation (43030, 393)'\n",
      "Clustering 43030 points in 393D to 500 clusters, redo 5 times, 500 iterations\n",
      "  Preprocessing in 0.01 s\n",
      "Outer iteration 0 / 5\n",
      "  Iteration 499 (63.13 s, search 62.07 s): objective=1.38885e+07 imbalance=1.760 nsplit=0       \n",
      "Objective improved: keep new clusters\n",
      "Outer iteration 1 / 5\n",
      "  Iteration 499 (127.06 s, search 125.00 s): objective=1.38864e+07 imbalance=1.772 nsplit=0       \n",
      "Objective improved: keep new clusters\n",
      "Outer iteration 2 / 5\n",
      "  Iteration 499 (190.51 s, search 187.42 s): objective=1.39157e+07 imbalance=1.902 nsplit=0       \n",
      "Outer iteration 3 / 5\n",
      "  Iteration 499 (253.96 s, search 249.84 s): objective=1.39172e+07 imbalance=1.832 nsplit=0       \n",
      "Outer iteration 4 / 5\n",
      "  Iteration 499 (317.29 s, search 312.15 s): objective=1.39139e+07 imbalance=1.848 nsplit=0       \n",
      "kmeans time: 317.43 s\n"
     ]
    }
   ],
   "source": [
    "#Estimate the optimal number of clusters as done in MAUVE\n",
    "num_of_clusters = max(2, int(round(min(len(embeddings_p)/10, len(embeddings_q)/10))))\n",
    "\n",
    "print(f'The number of clusters is {num_of_clusters}')\n",
    "results = mq.CDOE(torch.cat(embeddings_p), torch.cat(embeddings_q), num_of_clusters, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fb2c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining information to make future work easier\n",
    "\n",
    "p2cluster = results['p2cluster']\n",
    "for i in p2cluster:\n",
    "    ref[i]['cluster_id'] = p2cluster[i]\n",
    "q2cluster = results['q2cluster']\n",
    "for i in q2cluster:\n",
    "    remaining[i]['cluster_id'] = q2cluster[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "160887b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import dataset_scripts as ds_s, mauve_quantization as mq, subset_selection as ss\n",
    "p_distr = results['p_bin_counts']\n",
    "q_distr = results['q_bin_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e599417f-a235-4be4-879e-64106c79c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_human = [x for x in remaining if not x['under_collection']]\n",
    "q_clums = [x for x in remaining if x['under_collection']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a481183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "test_simple = ss.get_target_n_per_cluster(p_distr, q_distr, 5000, True)\n",
    "print(np.sum(list(test_simple.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21fb989e-43fe-4b89-b1b0-80d8f11be18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "test_complex = ss.get_target_n_per_cluster(p_distr, q_distr, 5000)\n",
    "print(np.sum(list(test_complex.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff1b5dd2-f664-4d22-9fd7-272968222667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the function to do a random subset that matches the 'target distribution'\n",
    "import random\n",
    "def random_subset_sampler(target_distribution, q2cluster):\n",
    "    cluster2q = {}\n",
    "    for key, value in q2cluster.items():\n",
    "        if value in cluster2q:\n",
    "            cluster2q[value].append(key)\n",
    "        else:\n",
    "            cluster2q[value] = [key]\n",
    "\n",
    "    returnable = []\n",
    "    for c in target_distribution:\n",
    "        if target_distribution[c] > 0:\n",
    "            returnable += random.sample(cluster2q[c], target_distribution[c])\n",
    "    return returnable\n",
    "\n",
    "test_sample = random_subset_sampler(test_simple, q2cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33f6605e-21c9-474c-95bd-5e70093c8ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_unders = [remaining[i]['under_collection_id'] for i in test_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98b7a0cc-d9cf-43de-bae2-d16520c2d4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of human texts in subset: 0.4972\n"
     ]
    }
   ],
   "source": [
    "print(f'Ratio of human texts in subset: {len([x for x in test_sample_unders if x])/len(test_sample_unders)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d7bf72a-2215-4063-af48-87c715ac0121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16  7 18 14 11 17 12 12  0  1 12  1  9  8 24  1  4 19  2 20  2 19  0  0\n",
      "  7  5  6  2  3  5 24 14  1  2  0  3  7  1 21  4 11  6 10 30  1 34 10  5\n",
      " 17  1 11 14 17 16  3  1  7 46  1  0 19  6 18 16  3  0 48  5 23  0  9 14\n",
      " 18 22  0 14 11  1 11 10 24 15  8  5 13 13  4 10  0  0  1 26 10  3 12 10\n",
      " 14 10  3  5  8 20 15 20 18  8 20  0 32  6 19  6  0 16  9  5 15  2  6  2\n",
      " 20  0 11  3 13 27  1 13  4  7 17 10 13  2 29 10  8  7 16 10  5 11 20 29\n",
      " 13  9 10  4  1  0 25  5  4  5  2  2  4  2 11 27  0 22 12  0  1 12 14  4\n",
      " 20 15  0 17  2 28  0  2  2 16 10 12  1  0  5  6  1 36 11  4 30  1  7  2\n",
      "  0  8 10  2 14  1  8  2  0  7  7 18 13  1 13  0  1  0 13  6  0  6  1  3\n",
      " 15 13  1  8 21  9  9 18 22  7 29  2 25  1  9  7  6 11 21  7  8 16  0  7\n",
      "  9  1  8  5 10  1 22  0 30  8 19 13  0  1  0  1  7 18  2 34 13  4  6 21\n",
      "  7 16 26 11  7  0  2 25  0 30  0 14  3 51  0  5  6 18  0  0  1 22 16  0\n",
      "  3  0  0  0 19  3 31 15 37  7  4  5 17  9 23 22 22 17 16 16  0 11  7  1\n",
      " 12 37 11 55  1  0 13  0  1 32  6  2  0 12  0 22  2 20  0 22 14 29  0 18\n",
      "  4  8  7 10  8 15  0 13  2  2 12  4 21  8  0  4  1  0  5  2  4  7  8  1\n",
      "  7 28  6  1 13  8  2  6 24 17 27  7  5 21 18 10  6  1 24  1 39 13 12  4\n",
      "  1  6 12  4  9  8 36  0  3  0 14  8  8  2  9 16 13 15 10 29  6  8  9  0\n",
      " 25  1 19 14  8  0 11  0  2  0  9  8 16 13  9  0  0  3  1  1 13  1  9  2\n",
      " 19  1 10  1 11  1 17 10  5  4  4 25  9 10 17  3 23  0 49  7  9 23 23  5\n",
      " 10  1 11  5 10 20  4  4 31 40  0  1 24  7 10  0  5  5 12 18 16  8  7 10\n",
      " 21  1  7  0  8 10 10 10  2 15  0  1  9  7  0 11  3  4 31 36]\n"
     ]
    }
   ],
   "source": [
    "print(results['p_bin_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29e182dc-fbee-4c42-b2d9-f39a5c4c2b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16  7 18 14 11 17 12 12  0  1 12  1  9  8 24  1  4 19  2 20  2 19  0  0\n",
      "  7  5  6  2  3  5 24 14  0  2  0  3  7  1 21  4 11  6 10 30  1 34 10  5\n",
      " 17  1 11 14 17 16  3  1  7 46  1  0 19  6 18 16  3  0 48  5 23  0  9 14\n",
      " 18 22  0 14 11  1 11 10 24 15  8  5 13 13  4 10  0  0  1 26 10  3 12 10\n",
      " 14 10  3  5  8 20 15 20 18  8 20  0 32  6 19  6  0 16  9  5 15  2  6  2\n",
      " 20  0 11  3 13 27  1 13  4  7 17 10 13  2 29 10  8  7 16 10  5 11 20 29\n",
      " 13  9 10  4  1  0 25  5  4  5  2  2  4  2 11 27  0 22 12  0  1 12 14  4\n",
      " 20 15  0 17  2 28  0  2  2 16 10 12  1  0  5  6  1 36 11  4 30  1  7  2\n",
      "  0  8 10  2 14  1  8  2  0  7  7 18 13  1 13  0  1  0 13  6  0  6  1  3\n",
      " 15 13  1  8 21  9  9 18 22  7 29  2 25  1  9  7  6 11 21  7  8 16  0  7\n",
      "  9  1  8  5 10  1 22  0 30  8 19 13  0  1  0  1  7 18  2 34 13  4  6 21\n",
      "  7 16 26 11  7  0  2 25  0 30  0 14  3 52  0  5  6 18  0  0  1 22 16  0\n",
      "  3  0  0  0 19  3 31 15 37  7  4  5 17  9 23 22 22 17 16 16  0 11  7  1\n",
      " 12 37 11 55  1  0 13  0  1 32  6  2  0 12  0 22  2 20  0 22 14 29  0 18\n",
      "  4  8  7 10  8 15  0 13  2  2 12  4 21  8  0  4  1  0  5  2  4  7  8  1\n",
      "  7 28  6  1 13  8  2  6 24 17 27  7  5 21 18 10  6  1 24  1 39 13 12  4\n",
      "  1  6 12  4  9  8 36  0  3  0 14  8  8  2  9 16 13 15 10 29  6  8  9  0\n",
      " 25  1 19 14  8  0 11  0  2  0  9  8 16 13  9  0  0  3  1  1 13  1  9  2\n",
      " 19  1 10  1 11  1 17 10  5  4  4 25  9 10 17  3 23  0 49  7  9 23 23  5\n",
      " 10  1 11  5 10 20  4  4 31 40  0  1 24  7 10  0  5  5 12 18 16  8  7 10\n",
      " 21  1  7  0  8 10 10 10  2 15  0  1  9  7  0 11  3  4 31 36]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(list(test_simple.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ee42c-78ee-4f36-91c9-e0818bcc7ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
