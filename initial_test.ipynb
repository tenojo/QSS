{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f15f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from scripts import dataset_scripts as ds_s, mauve_quantization as mq, subset_selection as ss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from datasets import Dataset, load_from_disk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e753dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicts are in format: {ds_path, ds_name, under_ds_name}\n",
    "#human_ds_dict = {\"ds_path\":\"data/human/news-fi-2019.jsonl\", \"ds_name\":\"news-fi-2019.jsonl\", \"under_ds_name\":None}\n",
    "#clums_ds_dict = {\"ds_path\":\"data/clumsified/news-fi-2019.jsonl_regeneration_5_mini_regen_round_1.jsonl\", \"ds_name\":\"news-fi-2019.jsonl_regeneration_5_mini_regen_round_1.jsonl\", \"under_ds_name\":\"news-fi-2019.jsonl\"}\n",
    "\n",
    "\n",
    "#ds = ds_s.format_datasets([human_ds_dict, clums_ds_dict])\n",
    "#ref, remaining = ds_s.sample_reference_corpus(ds, \"news-fi-2019.jsonl\", 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3dfcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/scratch/project_2000539/tapio/HF_cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d84c632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model name that can be loaded from HF goes here\n",
    "#model_id = model_name\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#model = AutoModel.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "\n",
    "# ## Getting the embeddings with the wanted (L)LM\n",
    "\n",
    "#inputs_q = [tokenizer.encode(x['text'], return_tensors=\"pt\", truncation=True, max_length=512) for x in remaining]\n",
    "#embeddings_q = mq.featurize_tokens_from_model(model, inputs_q, 1, name=\"\", verbose=False)\n",
    "#inputs_q = []\n",
    "#del inputs_q\n",
    "#inputs_p = [tokenizer.encode(x['text'], return_tensors=\"pt\", truncation=True, max_length=512) for x in ref]\n",
    "#embeddings_p = mq.featurize_tokens_from_model(model, inputs_p, 1, name=\"\", verbose=False)\n",
    "#inputs_p = []\n",
    "#del inputs_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4eac84-8b4c-4f5e-8732-f27d49711691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(ref)):\n",
    "#    ref[i]['embedding']= embeddings_p[i]\n",
    "\n",
    "#for i in range(len(remaining)):\n",
    "#    remaining[i]['embedding']= embeddings_q[i]\n",
    "\n",
    "\n",
    "#torch.save(ref, 'data/embs/ref.pt')\n",
    "\n",
    "#ds_test = Dataset.from_list(ref)\n",
    "#ds_test.save_to_disk(\"data/embs/ref.hf\")\n",
    "\n",
    "#ds_test = Dataset.from_list(remaining)\n",
    "#ds_test.save_to_disk(\"data/embs/remaining.hf\")\n",
    "\n",
    "#torch.load(remaining, 'data/embs/remaining.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5744e840-fac9-4f9c-8dae-0cff9eaa86c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"data/prepped_dataset/news_run_1_ppls.hf\").to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00dccd6c-1e70-4c81-b28c-af6574cc8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref, remaining = ds_s.sample_reference_corpus(ds, \"news-fi-2019.jsonl\", 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca1012d8-1dcf-4cb0-aa7d-91cf2539c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_p = [torch.tensor(x['embedding']) for x in ref]\n",
    "embeddings_q = [torch.tensor(x['embedding']) for x in remaining]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6834fd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of clusters is 500\n",
      "seed = 0\n",
      "performing clustering in lower dimension = 392\n",
      "'Shape of embeddings before PCA transformation (43030, 1024)'\n",
      "'Shape of embeddings after PCA transformation (43030, 393)'\n",
      "Clustering 43030 points in 393D to 500 clusters, redo 5 times, 500 iterations\n",
      "  Preprocessing in 0.01 s\n",
      "Outer iteration 0 / 5\n",
      "  Iteration 499 (111.15 s, search 107.35 s): objective=1.39086e+07 imbalance=1.841 nsplit=0       \n",
      "Objective improved: keep new clusters\n",
      "Outer iteration 1 / 5\n",
      "  Iteration 499 (220.03 s, search 213.10 s): objective=1.38641e+07 imbalance=1.722 nsplit=0       \n",
      "Objective improved: keep new clusters\n",
      "Outer iteration 2 / 5\n",
      "  Iteration 499 (328.43 s, search 318.35 s): objective=1.39168e+07 imbalance=1.883 nsplit=0       \n",
      "Outer iteration 3 / 5\n",
      "  Iteration 499 (446.06 s, search 429.54 s): objective=1.39213e+07 imbalance=1.816 nsplit=0       \n",
      "Outer iteration 4 / 5\n",
      "kmeans time: 560.82 s59 s, search 538.95 s): objective=1.39001e+07 imbalance=1.816 nsplit=0       \n"
     ]
    }
   ],
   "source": [
    "#Estimate the optimal number of clusters as done in MAUVE\n",
    "num_of_clusters = max(2, int(round(min(len(embeddings_p)/10, len(embeddings_q)/10))))\n",
    "\n",
    "print(f'The number of clusters is {num_of_clusters}')\n",
    "results = mq.CDOE(torch.cat(embeddings_p), torch.cat(embeddings_q), num_of_clusters, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb2c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining information to make future work easier\n",
    "\n",
    "p2cluster = results['p2cluster']\n",
    "for i in p2cluster:\n",
    "    ref[i]['cluster_id'] = int(p2cluster[i])\n",
    "q2cluster = results['q2cluster']\n",
    "for i in q2cluster:\n",
    "    remaining[i]['cluster_id'] = int(q2cluster[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06487526-b355-4462-b0aa-b8608f5c7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(array):\n",
    "        # Normalize sum of array to 1.\n",
    "        # We assume non-negative entries with non-zero sum.\n",
    "        return array / array.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "160887b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import dataset_scripts as ds_s, mauve_quantization as mq, subset_selection as ss\n",
    "p_distr = results['p_bin_counts']\n",
    "q_distr = np.histogram(\n",
    "        list(q2cluster.values()), bins=num_of_clusters,\n",
    "        range=[0, num_of_clusters], density=False\n",
    "    )[0]\n",
    "#q_distr = results['q_bin_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e599417f-a235-4be4-879e-64106c79c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_human = [x for x in remaining if not x['under_collection']]\n",
    "q_clums = [x for x in remaining if x['under_collection']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a481183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4998\n"
     ]
    }
   ],
   "source": [
    "test_simple = ss.get_target_n_per_cluster(p_distr, q_distr, 5000, True)\n",
    "print(np.sum(list(test_simple.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21fb989e-43fe-4b89-b1b0-80d8f11be18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "test_complex = ss.get_target_n_per_cluster(p_distr, q_distr, 5000)\n",
    "print(np.sum(list(test_complex.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff1b5dd2-f664-4d22-9fd7-272968222667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the function to do a random subset that matches the 'target distribution'\n",
    "import random\n",
    "def random_subset_sampler(target_distribution, q2cluster):\n",
    "    cluster2q = {}\n",
    "    for key, value in q2cluster.items():\n",
    "        if value in cluster2q:\n",
    "            cluster2q[value].append(key)\n",
    "        else:\n",
    "            cluster2q[value] = [key]\n",
    "\n",
    "    returnable = []\n",
    "    for c in target_distribution:\n",
    "        if target_distribution[c] > 0:\n",
    "            returnable += random.sample(cluster2q[c], target_distribution[c])\n",
    "    return returnable\n",
    "\n",
    "def ppl_ordered_subset_sampler(target_distribution, q2cluster, remaining, order_column):\n",
    "    cluster2q = {}\n",
    "    for key, value in q2cluster.items():\n",
    "        if value in cluster2q:\n",
    "            cluster2q[value].append(key)\n",
    "        else:\n",
    "            cluster2q[value] = [key]\n",
    "\n",
    "    returnable = []\n",
    "    for c in target_distribution:\n",
    "        if target_distribution[c] > 0:\n",
    "            sorted_embs = sorted(cluster2q[c], key=lambda index: remaining[index][order_column])\n",
    "            returnable += sorted_embs[:target_distribution[c]]\n",
    "    return returnable\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33f6605e-21c9-474c-95bd-5e70093c8ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of human texts in subset (using simple per-cluster counts): 0.4943977591036415\n"
     ]
    }
   ],
   "source": [
    "test_sample = random_subset_sampler(test_simple, q2cluster)\n",
    "test_sample_unders = [remaining[i]['under_collection_id'] for i in test_sample]\n",
    "print(f'Ratio of human texts in subset (using simple per-cluster counts): {len([x for x in test_sample_unders if x])/len(test_sample_unders)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98b7a0cc-d9cf-43de-bae2-d16520c2d4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of human texts in subset when ordered by ppl (using complex per-cluster counts): 0.6572629051620649\n"
     ]
    }
   ],
   "source": [
    "test_sample = ppl_ordered_subset_sampler(test_simple, q2cluster, remaining, 'cppl')\n",
    "test_sample_unders = [remaining[i]['under_collection_id'] for i in test_sample]\n",
    "print(f'Ratio of human texts in subset when ordered by ppl (using complex per-cluster counts): {len([x for x in test_sample_unders if x])/len(test_sample_unders)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "274e5395-e1c5-483d-a3a7-dfbec86d82f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of human texts in subset (using simple per-cluster counts): 0.4992\n"
     ]
    }
   ],
   "source": [
    "test_sample = random_subset_sampler(test_complex, q2cluster)\n",
    "test_sample_unders = [remaining[i]['under_collection_id'] for i in test_sample]\n",
    "print(f'Ratio of human texts in subset (using simple per-cluster counts): {len([x for x in test_sample_unders if x])/len(test_sample_unders)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e4241a3-5efd-448f-8107-bec435949c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of human texts in subset when ordered by ppl (using complex per-cluster counts): 0.6572\n"
     ]
    }
   ],
   "source": [
    "test_sample = ppl_ordered_subset_sampler(test_complex, q2cluster, remaining, 'cppl')\n",
    "test_sample_unders = [remaining[i]['under_collection_id'] for i in test_sample]\n",
    "print(f'Ratio of human texts in subset when ordered by ppl (using complex per-cluster counts): {len([x for x in test_sample_unders if x])/len(test_sample_unders)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcbdc6b-3888-4020-a24d-c8bcdad8ccc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
